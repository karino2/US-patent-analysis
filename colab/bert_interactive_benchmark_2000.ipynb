{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-interactive-benchmark-2000.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karino2/US-patent-analysis/blob/master/colab/bert_interactive_benchmark_2000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rQph610BeO4I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Interacitve evaluation notebook for BERT with grants2000**\n",
        "\n",
        "Input app claim (from 2017, cited from 2012) and show most similar claim."
      ]
    },
    {
      "metadata": {
        "id": "3Cv0EJiZ8vi6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R3AyT0Fq8yzD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3cM2dZ__NPt1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone -b docker https://github.com/yoheikikuta/bert.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eg8r8ARt89w3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgPiQOJRMgxt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data setup\n",
        "\n",
        "Data setup is based on https://github.com/karino2/US-patent-analysis/blob/bert_input/colab/bert_tokenized_claim_setup.ipynb\n",
        "\n",
        "We only use grants tokens. (though file both contains applications and grants)"
      ]
    },
    {
      "metadata": {
        "id": "XG9_uLXKMf2t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir ./bert/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j-n-oE9aNPs6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zn-onntHM0lN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://karino2-uspatent/features/test_grants_ids.pkl.gz ./bert/data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kywCunsPQxjZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls ./bert/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S1jf0kzIM0Ss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with gzip.open(\"./bert/data/test_grants_ids.pkl.gz\", 'rb') as f:\n",
        "     (_, grants_ids) = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nN0LX7jKOAdi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8vJgj4gZN_-_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://karino2-uspatent/citations_info_2000.df.gz ./bert/data/\n",
        "!gsutil cp gs://karino2-uspatent/testset_app_1000.df.gz ./bert/data/\n",
        "!gsutil cp gs://karino2-uspatent/training_app_1000.df.gz ./bert/data/\n",
        "!gsutil cp gs://karino2-uspatent/grants_for_2000.df.gz ./bert/data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CJC9Q6MjOHZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "citations_info_target = pd.read_pickle(\"./bert/data/citations_info_2000.df.gz\")\n",
        "# training_app_df = pd.read_pickle(\"./bert/data/training_app_1000.df.gz\")\n",
        "testset_app_df = pd.read_pickle(\"./bert/data/testset_app_1000.df.gz\")\n",
        "grants_target_df = pd.read_pickle(\"./bert/data/grants_for_2000.df.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_oPSPIxXM0Ba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yLWTT6NZ_ZYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Check tpu name**"
      ]
    },
    {
      "metadata": {
        "id": "pShRjorw_Lla",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lBOhgRrYGJzS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bert setup"
      ]
    },
    {
      "metadata": {
        "id": "D9hQGFJRwbBl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VOCAB_CONFIG_PATH=' gs://yohei-kikuta/mlstudy-phys/bert/models/pre-trained-models/uncased_L-12_H-768_A-12' #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6hfcrTkmwNJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp {VOCAB_CONFIG_PATH}/vocab.txt ./bert/model/patent/vocab.txt\n",
        "!gsutil cp {VOCAB_CONFIG_PATH}/bert_config.json ./bert/model/patent/bert_config.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWPKTaJ8viYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls ./bert/model/patent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_7OoVrSLxX2i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LpSs4ff-xXc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./bert\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aiCmZuxgN2rx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZtR55qoWN2Uy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = tokenization.FullTokenizer(\n",
        "    vocab_file=\"./bert/model/patent/vocab.txt\", do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgyWpBn-N99_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CLS_ID = tokenizer.vocab[\"[CLS]\"]\n",
        "SEP_ID = tokenizer.vocab[\"[SEP]\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7eF3NHnxN9v8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5CLJu1tSKe5S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s1IMQ8o6su3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_list = [\"not_entailment\", \"entailment\"]\n",
        "max_seq_length = 512\n",
        "\n",
        "label_map = {}\n",
        "for (i, label) in enumerate(label_list):\n",
        "  label_map[label] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1KJ2xS5aI1G4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def original_convert_single_pair(ids_a_input, ids_b_input):\n",
        "\n",
        "  # Clone and pop for truncate. Most of the case result len is the same for our purpose, but try simple implementation first.\n",
        "  ids_a = list(ids_a_input)\n",
        "  ids_b = list(ids_b_input)\n",
        "    \n",
        "  # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "  # length is less than the specified length.\n",
        "  # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "  _truncate_seq_pair(ids_a, ids_b, max_seq_length - 3)\n",
        "\n",
        "  # The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        "  input_ids = []\n",
        "  segment_ids = []\n",
        "  input_ids.append(CLS_ID)\n",
        "  segment_ids.append(0)\n",
        "  for token in ids_a:\n",
        "    input_ids.append(token)\n",
        "    segment_ids.append(0)\n",
        "  input_ids.append(SEP_ID)\n",
        "  segment_ids.append(0)\n",
        "\n",
        "  for token in ids_b:\n",
        "    input_ids.append(token)\n",
        "    segment_ids.append(1)\n",
        "  input_ids.append(SEP_ID)\n",
        "  segment_ids.append(1)\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "\n",
        "  return (input_ids, input_mask, segment_ids)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ka67cUqvrzSn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimization for our application\n",
        "\n",
        "dup ids consume huge memory.\n",
        "We cach and use the same memory if ids_a_nput and ids_b_input have enough size."
      ]
    },
    {
      "metadata": {
        "id": "vS4V68lnteSp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NORMAL_INPUT_MASK = [1]*max_seq_length\n",
        "NORMAL_SEGMENT_IDS = [0]*257+[1]*255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WepvGg7GzBHN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jU7GTdsDu3pr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# [CLS](0), 1-255(len=255), [SEP], 257-510(len=254), [SEP]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qEiIH_OYvdoi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LEN_255_CACHE = {}\n",
        "LEN_254_CACHE = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3MXXhPIlwnpp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def truncage_with_cache(ids, target_len):\n",
        "  if target_len == 255:\n",
        "    cache = LEN_255_CACHE\n",
        "  else:\n",
        "    assert target_len == 254\n",
        "    cache = LEN_254_CACHE\n",
        "  if ids not in cache:\n",
        "    cache[ids] = ids[0:target_len]\n",
        "  return cache[ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-aZoDPL9rtr_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fast_convert_single_pair(ids_a_input, ids_b_input):  \n",
        "  if (len(ids_a_input) < 256) or (len(ids_b_input) < 256) :\n",
        "    return original_convert_single_pair(ids_a_input, ids_b_input)\n",
        "  \n",
        "  ids_a = truncage_with_cache(ids_a_input, 255)\n",
        "  ids_b = truncage_with_cache(ids_b_input, 254)\n",
        "\n",
        "  input_ids = (CLS_ID,) + ids_a + (SEP_ID,) + ids_b + (SEP_ID,)\n",
        "  \n",
        "  return (input_ids, NORMAL_INPUT_MASK, NORMAL_SEGMENT_IDS)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ODA4cgTN6nu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feature_tuplist_to_feature_dict(ftups):\n",
        "    dic = {}\n",
        "    dic['input_ids'] = [tup[0] for tup in ftups]\n",
        "    dic[\"input_mask\"] = [tup[1] for tup in ftups]\n",
        "    dic[\"segment_ids\"] = [tup[2] for tup in ftups]\n",
        "    dic[\"label_ids\"] = [[0] for _ in ftups]\n",
        "    return dic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qr6ULqjyzVHC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert to tuple for optimization.\n",
        "grants_ids_tup = [tuple(ids) for ids in grants_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EpPg7jBjNzYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tokenize claim"
      ]
    },
    {
      "metadata": {
        "id": "EbuFVKieOfpQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "CLAIM_PAT = re.compile(r'<claims[^>]*>(.*)</claims>',re.MULTILINE|re.DOTALL)\n",
        "TAG_PAT = re.compile(r\"<.*?>\")\n",
        "\n",
        "\n",
        "def whole_xml_to_claim_xml(whole):\n",
        "    mat = CLAIM_PAT.search(whole)\n",
        "    return mat.group(1)\n",
        "  \n",
        "  \n",
        "def whole_xml_to_claim(whole):\n",
        "    return TAG_PAT.sub(' ', whole_xml_to_claim_xml(whole))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wLmSPcs6N4RZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def claim_to_ids(claim):\n",
        "  return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(claim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O473b-9RnnZ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model setup"
      ]
    },
    {
      "metadata": {
        "id": "bgGldgUEN4cD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_9U0S-hYxeMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import modeling\n",
        "import tokenization\n",
        "\n",
        "from run_classifier import model_fn_builder\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AburDQpQxd_k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bert_config = modeling.BertConfig.from_json_file(\"./bert/model/patent/bert_config.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-hGfDjS6x7fw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BUCKET = 'karino2-uspatent' #@param {type:\"string\"}\n",
        "OUTPUT_DIR = 'gs://{}/mlstudy-phys/bert/models/patent_1000'.format(BUCKET)\n",
        "\n",
        "INIT_CHECK_POINT_FILE='model.ckpt-1602' #@param {type:\"string\"}\n",
        "INIT_CHECK_POINT = '{}/{}'.format(OUTPUT_DIR, INIT_CHECK_POINT_FILE)\n",
        "\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "print('***** Init checkpoint: {} *****'.format(INIT_CHECK_POINT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7yRP7fE5qu2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OISS39WWxdxv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FLAGS(object):\n",
        "    '''Parameters.'''\n",
        "    def __init__(self):\n",
        "        self.vocab_file = \"./bert/model/patent/vocab.txt\"\n",
        "        self.use_tpu = True\n",
        "        self.output_dir = OUTPUT_DIR\n",
        "        # self.data_dir = \"./bert/data/1000\"\n",
        "        self.init_checkpoint = INIT_CHECK_POINT\n",
        "        self.predict_batch_size = 16\n",
        "        # TPU related\n",
        "        self.num_tpu_cores = 8\n",
        "        self.tpu_name = TPU_ADDRESS\n",
        "        \n",
        "        # following parameters are not used anymore. (because we create feature by hand)\n",
        "        self.do_lower_case = True\n",
        "        self.max_seq_length = 512\n",
        "        \n",
        "        # The following parameters are not used in predictions.\n",
        "        # Just use to create RunConfig.\n",
        "        self.master = None\n",
        "        self.save_checkpoints_steps = 1\n",
        "        self.learning_rate = 0\n",
        "        self.num_warmup_steps = 0\n",
        "        self.num_train_steps = 0\n",
        "        # TPU related. Some of these value have positive int not to make TPUEstimator angry (even though these are not used...).\n",
        "        self.eval_batch_size = 32\n",
        "        self.train_batch_size = 32 \n",
        "        self.iterations_per_loop = 1000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YjiBu4fzxdjY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FLAGS = FLAGS()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wvl80s6_zH_T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tokenizer = tokenization.FullTokenizer(\n",
        "#    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
        "\n",
        "# tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu_name).get_master()\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu_name)\n",
        "\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    master=FLAGS.master,\n",
        "    model_dir=FLAGS.output_dir,\n",
        "    save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "        num_shards=FLAGS.num_tpu_cores,\n",
        "        per_host_input_for_training=is_per_host))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "icN9SGPpzHxe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_fn = model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    num_labels=len([\"not_entailment\", \"entailment\"]),\n",
        "    init_checkpoint=FLAGS.init_checkpoint,\n",
        "    learning_rate=FLAGS.learning_rate,\n",
        "    num_train_steps=FLAGS.num_train_steps,\n",
        "    num_warmup_steps=FLAGS.num_warmup_steps,\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    use_one_hot_embeddings=FLAGS.use_tpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YOnW40GE9yTU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/kyzhouhzau/BERT-NER/issues/19\n",
        "\n",
        "def model_fn_wrapper(features, labels, mode, params):\n",
        "  tspec = model_fn(features, labels, mode, params)\n",
        "  pred_dict = {'predictions': tspec.predictions}\n",
        "  return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=tspec.mode, predictions=pred_dict, scaffold_fn=tspec.scaffold_fn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rPgJuC0A9yAq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    # model_fn=model_fn,\n",
        "    model_fn=model_fn_wrapper,\n",
        "    config=run_config,\n",
        "    train_batch_size=FLAGS.train_batch_size,\n",
        "    eval_batch_size=FLAGS.eval_batch_size,\n",
        "    predict_batch_size=FLAGS.predict_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0neZwVmuoZ6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nXqSH6HmoZrJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_fdicts(fdict):\n",
        "  def _batch_input_function(params):\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices(fdict)    \n",
        "    return test_ds.batch(params['batch_size'])\n",
        "  \n",
        "  result = estimator.predict(_batch_input_function)\n",
        "  print(datetime.datetime.today())\n",
        "  result = list(result)\n",
        "  print(datetime.datetime.today())\n",
        "  \n",
        "  return  [pred['predictions'] for pred in result]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EtOaMQvloZSz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bKK_MOdEPHUj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Interactive view"
      ]
    },
    {
      "metadata": {
        "id": "2DTtgwRTuvSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def results_to_result_df(results):\n",
        "  results_arr = np.array(results)\n",
        "  results_df = grants_target_df.copy()\n",
        "  results_df[\"score\"] = results_arr[:, 1]\n",
        "  return results_df.sort_values(\"score\", ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yw2POytHbQ6R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import widgets\n",
        "from google.colab import output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Anj_wKJdvHkP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "selected_result_claim = \"\"\n",
        "\n",
        "def show_candidates(result_df, app_id):\n",
        "  global selected_result_claim\n",
        "  print('Show 10 most probable candidate for app({})'.format(app_id))\n",
        "\n",
        "  candidate_df = result_df.sort_values(\"score\", ascending=False)[0:10]\n",
        "\n",
        "\n",
        "  tab_labels = list(candidate_df.apply(lambda row: \"{}:{:.6f}\".format(row.parsed, row.score), axis=1))\n",
        "\n",
        "  tb = widgets.TabBar(tab_labels, location=\"top\")\n",
        "  for i, _ in enumerate(tab_labels):\n",
        "    with tb.output_to(i):\n",
        "      selected_result_claim = whole_xml_to_claim(candidate_df[\"xml\"].values[i])\n",
        "      print(selected_result_claim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFXBo084xThx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### How to use\n",
        "\n",
        "1. Eval cell below (the result is random sampled every time you eval)\n",
        "2. Read the claim from tabs and decide the application claim which you want to search the cited candidate for.\n",
        "3. Choose tab index from dropdown below to specify search candidate (left most tab corresponds to 0)\n",
        "4. Eval cell below of the dropdown cell. This eval takes about 40 seconds.\n",
        "5. See the results."
      ]
    },
    {
      "metadata": {
        "id": "7Z9CKgdlZYrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Random 10 sampling of application claims. (re-sampling each time cell eval)')\n",
        "\n",
        "\n",
        "sampled_apps = testset_app_df.sample(10)\n",
        "\n",
        "\n",
        "app_ids = [f\"({id}) {title}\" for id, title in enumerate(sampled_apps.app_id.values)]\n",
        "tb = widgets.TabBar(app_ids, location=\"top\")\n",
        "for i, _ in enumerate(app_ids):\n",
        "  with tb.output_to(i):\n",
        "    print(whole_xml_to_claim(sampled_apps[\"xml\"].values[i]))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V8OTh9iljZOy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title 検索する出願のインデックス（上記タブの0から9番目のどれか）を選ぶ { run: \"auto\", vertical-output: true }\n",
        "\n",
        "target_claim_index = \"6\" #@param [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# Selected result becomes string...\n",
        "target_claim_idx = int(target_claim_index)\n",
        "print(\"target app: {}\".format(app_ids[target_claim_idx]))\n",
        "target_claim = whole_xml_to_claim(sampled_apps[\"xml\"].values[target_claim_idx])\n",
        "print(\"claim: {}...\".format(target_claim[0:100]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMBu0etUS7uj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jg-NxBHMkTLh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "target_claim_ids = tuple(claim_to_ids(target_claim))\n",
        "pairs = [(target_claim_ids, grants_claim) for grants_claim in grants_ids_tup]\n",
        "fdicts = feature_tuplist_to_feature_dict([fast_convert_single_pair(tup[0], tup[1]) for tup in pairs])\n",
        "\n",
        "# The below line takes about 40 seconds\n",
        "results = predict_fdicts(fdicts)\n",
        "\n",
        "results_df = results_to_result_df(results)\n",
        "show_candidates(results_df, app_ids[target_claim_idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "88t4trt-UpPJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Show what is the model input"
      ]
    },
    {
      "metadata": {
        "id": "Dutf-m5OM5jN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Input claim (application)\")\n",
        "print(target_claim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mmK0LrTuM5Ts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(target_claim)\n",
        "for idx, token in enumerate(tokens[0:255]):\n",
        "  sep = \"\\n\" if (idx != 0 and idx%15 == 0) else \" \"\n",
        "  print(token, end=sep)\n",
        "print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZcUj20-zMt7Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Selected claim (grants)\")\n",
        "print(selected_result_claim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JK9jkisSMvBX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(selected_result_claim)\n",
        "for idx, token in enumerate(tokens[0:254]):\n",
        "  sep = \"\\n\" if (idx != 0 and idx%15 == 0) else \" \"\n",
        "  print(token, end=sep)\n",
        "print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxTDvVWZOaw4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}